% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{Hiesinger2023}
W.~Hiesinger, C.~Zakka, A.~Chaurasia, R.~Shad, A.~R. Dalal, J.~Kim, M.~Moor,
  K.~Alexander, E.~A. Ashley, J.~Boyd, K.~Boyd, K.~Hirsch, C.~Langlotz, and
  J.~Nelson, ``Almanac: Retrieval-augmented language models for clinical
  medicine,'' \emph{Research Square}, 2023.

\bibitem{Jin2021}
Q.~Jin, Z.~Yuan, G.~Xiong, Q.~Yu, H.~Ying, C.~Tan, M.~Chen, S.~Huang, X.~Liu,
  and S.~Yu, ``Biomedical question answering: A survey of approaches and
  challenges,'' \emph{ACM Computing Surveys (CSUR)}, vol.~55, pp. 1--36, 2021.

\bibitem{Kalyan2021}
K.~S. Kalyan, A.~Rajasekharan, and S.~Sangeetha, ``Ammu - a survey of
  transformer-based biomedical pretrained language models,'' \emph{ArXiv}, vol.
  abs/2105.00827, 2021.

\bibitem{Tinn2021}
R.~Tinn, H.~Cheng, Y.~Gu, N.~Usuyama, X.~Liu, T.~Naumann, J.~Gao, and H.~Poon,
  ``Fine-tuning large neural language models for biomedical natural language
  processing,'' \emph{Patterns}, 2021.

\bibitem{Tran2023}
H.~Tran, Z.~Yang, Z.~Yao, and H.~Yu, ``Bioinstruct: Instruction tuning of large
  language models for biomedical natural language processing,'' \emph{ArXiv},
  2023.

\bibitem{Wang2023}
Q.~Wang, Z.~Gao, and R.~Xu, ``Exploring the in-context learning ability of
  large language model for biomedical concept linking,'' \emph{ArXiv}, 2023.

\bibitem{Wang2023a}
G.~Wang, G.~Yang, Z.~Du, L.~Fan, and X.~Li, ``Clinicalgpt: Large language
  models finetuned with diverse medical data and comprehensive evaluation,''
  \emph{ArXiv}, 2023.

\bibitem{Ozyurt2020}
I.~B. Ozyurt, ``On the effectiveness of small, discriminatively pre-trained
  language representation models for biomedical text mining,'' \emph{bioRxiv},
  2020.

\bibitem{Chen2023}
S.~Chen, Y.~Li, S.~Lu, H.~Van, H.~Aerts, G.~Savova, and D.~Bitterman,
  ``Evaluation of chatgpt family of models for biomedical reasoning and
  classification,'' \emph{ArXiv}, 2023.

\bibitem{Suster2018}
S.~Suster and W.~Daelemans, ``Clicr: a dataset of clinical case reports for
  machine reading comprehension,'' \emph{ArXiv}, vol. abs/1803.09720, 2018.

\bibitem{Soman2023}
K.~Soman, P.~W. Rose, J.~H. Morris, R.~E. Akbas, B.~Smith, B.~Peetoom,
  C.~Villouta-Reyes, G.~Cerono, Y.~Shi, A.~Rizk-Jackson, S.~Israni, C.~A.
  Nelson, S.~Huang, and S.~Baranzini, ``Biomedical knowledge graph-enhanced
  prompt generation for large language models,'' \emph{ArXiv}, vol.
  abs/2311.17330, 2023.

\bibitem{Gu2020}
Y.~Gu, R.~Tinn, H.~Cheng, M.~R. Lucas, N.~Usuyama, X.~Liu, T.~Naumann, J.~Gao,
  and H.~Poon, ``Domain-specific language model pretraining for biomedical
  natural language processing,'' \emph{ACM Transactions on Computing for
  Healthcare (HEALTH)}, vol.~3, pp. 1 -- 23, 2020.

\bibitem{Jeong2020}
M.~Jeong, M.~Sung, G.~Kim, D.~Kim, W.~Yoon, J.~Yoo, and J.~Kang,
  ``Transferability of natural language inference to biomedical question
  answering,'' \emph{ArXiv}, vol. abs/2007.00217, 2020.

\bibitem{Miolo2021}
G.~Miolo, G.~Mantoan, and C.~Orsenigo, ``Electramed: a new pre-trained language
  representation model for biomedical nlp,'' \emph{ArXiv}, vol. abs/2104.09585,
  2021.

\bibitem{Duong2023}
D.~Duong and B.~D. Solomon, ``Analysis of large-language model versus human
  performance for genetics questions,'' \emph{medRxiv: the preprint server for
  health sciences}, 2023.

\bibitem{Lee2023}
D.~T. Lee, A.~Vaid, B.~M. Menon, R.~R. Freeman, D.~S. Matteson, M.~P. Marin,
  and G.~N. Nadkarni, ``Development of a privacy preserving large language
  model for automated data extraction from thyroid cancer pathology reports,''
  \emph{medRxiv}, 2023.

\bibitem{Schubert2023}
M.~Schubert, W.~Wick, and V.~Venkataramani, ``Performance of large language
  models on a neurology board–style examination,'' \emph{JAMA Network Open},
  vol.~6, 2023.

\bibitem{Kumari2023}
A.~Kumari, A.~Kumari, A.~Singh, S.~Singh, A.~Juhi, A.~Dhanvijay, M.~Pinjar, and
  H.~Mondal, ``Large language models in hematology case solving: A comparative
  study of chatgpt-3.5, google bard, and microsoft bing,'' \emph{Cureus},
  vol.~15, 2023.

\bibitem{Koga2023}
S.~Koga, ``Exploring the pitfalls of large language models: Inconsistency and
  inaccuracy in answering pathology board examination‐style questions,''
  \emph{Pathology International}, vol.~73, 2023.

\bibitem{Reese2023}
J.~Reese, D.~Danis, J.~H. Caulfied, E.~Casiraghi, G.~Valentini, C.~Mungall, and
  P.~N. Robinson, ``On the limitations of large language models in clinical
  diagnosis,'' \emph{medRxiv}, 2023.

\bibitem{Pal2023}
R.~Pal, H.~Garg, S.~Patel, and T.~Sethi, ``Bias amplification in intersectional
  subpopulations for clinical phenotyping by large language models,''
  \emph{medRxiv}, 2023.

\bibitem{Buckley2023}
T.~A. Buckley, J.~A. Diao, A.~Rodman, and A.~K. Manrai, ``Accuracy of a
  vision-language model on challenging medical cases,'' \emph{ArXiv}, vol.
  abs/2311.05591, 2023.

\bibitem{Titus2023}
A.~J. Titus, ``Nhanes-gpt: Large language models (llms) and the future of
  biostatistics,'' \emph{medRxiv}, 2023.

\bibitem{Holmes2023a}
J.~Holmes, S.~Ye, Y.~Li, S.-N. Wu, Z.~Liu, Z.~Wu, J.~Hu, H.~Zhao, X.~Jiang,
  W.~Liu, H.~Wei, J.~Zou, T.~Liu, and Y.~Shao, ``Evaluating large language
  models in ophthalmology,'' \emph{ArXiv}, vol. abs/2311.04933, 2023.

\bibitem{Holmes2023b}
J.~Holmes, R.~Peng, Y.~Li, J.~Hu, Z.~Liu, Z.~Wu, H.~Zhao, X.~Jiang, W.~Liu,
  H.~Wei, J.~Zou, T.~Liu, and Y.~Shao, ``Evaluating multiple large language
  models in pediatric ophthalmology,'' \emph{ArXiv}, vol. abs/2311.04368, 2023.

\bibitem{Holmes2023c}
J.~Holmes, Z.~Liu, L.-C. Zhang, Y.~Ding, T.~Sio, L.~Mcgee, J.~Ashman, X.~Li,
  T.~Liu, J.~Shen, and W.~Liu, ``Evaluating large language models on a
  highly-specialized topic, radiation oncology physics,'' \emph{Frontiers in
  Oncology}, vol.~13, 2023.

\bibitem{Wang2022}
Z.~Wang, ``Modern question answering datasets and benchmarks: A survey,''
  \emph{ArXiv}, vol. abs/2206.15030, 2022.

\bibitem{Chowdhury2023}
M.~Chowdhury, E.~Lim, A.~Higham, R.~McKinnon, N.~Ventoura, Y.~He, and N.~D.
  Pennington, ``Can large language models safely address patient questions
  following cataract surgery?'' \emph{Clinical NLP}, vol.~1, pp. 131--137,
  2023.

\bibitem{Li2024}
J.~Li, S.~Wang, M.~Zhang, W.~Li, Y.~Lai, and X.~Kang, ``Agent hospital: A
  simulacrum of hospital with evolvable medical agents,'' 2024.

\bibitem{openai2024gpt4}
OpenAI, J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman,
  D.~Almeida, J.~Altenschmidt, S.~Altman, S.~Anadkat, R.~Avila, I.~Babuschkin,
  S.~Balaji, V.~Balcom, P.~Baltescu, H.~Bao, M.~Bavarian, J.~Belgum, I.~Bello,
  J.~Berdine, G.~Bernadett-Shapiro, C.~Berner, L.~Bogdonoff, O.~Boiko, M.~Boyd,
  A.-L. Brakman, G.~Brockman, T.~Brooks, M.~Brundage, K.~Button, T.~Cai,
  R.~Campbell, A.~Cann, B.~Carey, C.~Carlson, R.~Carmichael, B.~Chan, C.~Chang,
  F.~Chantzis, D.~Chen, S.~Chen, R.~Chen, J.~Chen, M.~Chen, B.~Chess, C.~Cho,
  C.~Chu, H.~W. Chung, D.~Cummings, J.~Currier, Y.~Dai, C.~Decareaux, T.~Degry,
  N.~Deutsch, D.~Deville, A.~Dhar, D.~Dohan, S.~Dowling, S.~Dunning,
  A.~Ecoffet, A.~Eleti, T.~Eloundou, D.~Farhi, L.~Fedus, N.~Felix, S.~P.
  Fishman, J.~Forte, I.~Fulford, L.~Gao, E.~Georges, C.~Gibson, V.~Goel,
  T.~Gogineni, G.~Goh, R.~Gontijo-Lopes, J.~Gordon, M.~Grafstein, S.~Gray,
  R.~Greene, J.~Gross, S.~S. Gu, Y.~Guo, C.~Hallacy, J.~Han, J.~Harris, Y.~He,
  M.~Heaton, J.~Heidecke, C.~Hesse, A.~Hickey, W.~Hickey, P.~Hoeschele,
  B.~Houghton, K.~Hsu, S.~Hu, X.~Hu, J.~Huizinga, S.~Jain, S.~Jain, J.~Jang,
  A.~Jiang, R.~Jiang, H.~Jin, D.~Jin, S.~Jomoto, B.~Jonn, H.~Jun, T.~Kaftan,
  Łukasz Kaiser, A.~Kamali, I.~Kanitscheider, N.~S. Keskar, T.~Khan,
  L.~Kilpatrick, J.~W. Kim, C.~Kim, Y.~Kim, J.~H. Kirchner, J.~Kiros,
  M.~Knight, D.~Kokotajlo, Łukasz Kondraciuk, A.~Kondrich, A.~Konstantinidis,
  K.~Kosic, G.~Krueger, V.~Kuo, M.~Lampe, I.~Lan, T.~Lee, J.~Leike, J.~Leung,
  D.~Levy, C.~M. Li, R.~Lim, M.~Lin, S.~Lin, M.~Litwin, T.~Lopez, R.~Lowe,
  P.~Lue, A.~Makanju, K.~Malfacini, S.~Manning, T.~Markov, Y.~Markovski,
  B.~Martin, K.~Mayer, A.~Mayne, B.~McGrew, S.~M. McKinney, C.~McLeavey,
  P.~McMillan, J.~McNeil, D.~Medina, A.~Mehta, J.~Menick, L.~Metz,
  A.~Mishchenko, P.~Mishkin, V.~Monaco, E.~Morikawa, D.~Mossing, T.~Mu,
  M.~Murati, O.~Murk, D.~Mély, A.~Nair, R.~Nakano, R.~Nayak, A.~Neelakantan,
  R.~Ngo, H.~Noh, L.~Ouyang, C.~O'Keefe, J.~Pachocki, A.~Paino, J.~Palermo,
  A.~Pantuliano, G.~Parascandolo, J.~Parish, E.~Parparita, A.~Passos,
  M.~Pavlov, A.~Peng, A.~Perelman, F.~de~Avila Belbute~Peres, M.~Petrov, H.~P.
  de~Oliveira~Pinto, Michael, Pokorny, M.~Pokrass, V.~H. Pong, T.~Powell,
  A.~Power, B.~Power, E.~Proehl, R.~Puri, A.~Radford, J.~Rae, A.~Ramesh,
  C.~Raymond, F.~Real, K.~Rimbach, C.~Ross, B.~Rotsted, H.~Roussez, N.~Ryder,
  M.~Saltarelli, T.~Sanders, S.~Santurkar, G.~Sastry, H.~Schmidt, D.~Schnurr,
  J.~Schulman, D.~Selsam, K.~Sheppard, T.~Sherbakov, J.~Shieh, S.~Shoker,
  P.~Shyam, S.~Sidor, E.~Sigler, M.~Simens, J.~Sitkin, K.~Slama, I.~Sohl,
  B.~Sokolowsky, Y.~Song, N.~Staudacher, F.~P. Such, N.~Summers, I.~Sutskever,
  J.~Tang, N.~Tezak, M.~B. Thompson, P.~Tillet, A.~Tootoonchian, E.~Tseng,
  P.~Tuggle, N.~Turley, J.~Tworek, J.~F.~C. Uribe, A.~Vallone, A.~Vijayvergiya,
  C.~Voss, C.~Wainwright, J.~J. Wang, A.~Wang, B.~Wang, J.~Ward, J.~Wei,
  C.~Weinmann, A.~Welihinda, P.~Welinder, J.~Weng, L.~Weng, M.~Wiethoff,
  D.~Willner, C.~Winter, S.~Wolrich, H.~Wong, L.~Workman, S.~Wu, J.~Wu, M.~Wu,
  K.~Xiao, T.~Xu, S.~Yoo, K.~Yu, Q.~Yuan, W.~Zaremba, R.~Zellers, C.~Zhang,
  M.~Zhang, S.~Zhao, T.~Zheng, J.~Zhuang, W.~Zhuk, and B.~Zoph, ``Gpt-4
  technical report,'' 2024.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozière, N.~Goyal, E.~Hambro, F.~Azhar, A.~Rodriguez, A.~Joulin,
  E.~Grave, and G.~Lample, ``Llama: Open and efficient foundation language
  models,'' 2023.

\bibitem{jiang2023mistral}
A.~Q. Jiang, A.~Sablayrolles, A.~Mensch, C.~Bamford, D.~S. Chaplot, D.~de~las
  Casas, F.~Bressand, G.~Lengyel, G.~Lample, L.~Saulnier, L.~R. Lavaud, M.-A.
  Lachaux, P.~Stock, T.~L. Scao, T.~Lavril, T.~Wang, T.~Lacroix, and W.~E.
  Sayed, ``Mistral 7b,'' 2023.

\bibitem{cohere2023aya}
A.~Üstün, V.~Aryabumi, Z.-X. Yong, W.-Y. Ko, D.~D'souza, G.~Onilude,
  N.~Bhandari, S.~Singh, H.-L. Ooi, A.~Kayid, F.~Vargus, P.~Blunsom,
  S.~Longpre, N.~Muennighoff, M.~Fadaee, J.~Kreutzer, and S.~Hooker, ``Aya
  model: An instruction finetuned open-access multilingual language model,''
  2024.

\end{thebibliography}
